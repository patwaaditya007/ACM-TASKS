{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\n",
    "Resources:\n",
    "* https://youtu.be/aircAruvnKk\n",
    "* http://neuralnetworksanddeeplearning.com/\n",
    "* playground.tensorflow.org\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "TASK 1\n",
    "INSTRUCTIONS:\n",
    "There are 11 TODOS in this python file\n",
    "Fill each one of those appropriately and you will have a working neural network\n",
    "Instructions and resources have been provided wherever possible.\n",
    "The implementation may not be perfect, so feel free to point out any mistakes / ask any doubts\n",
    "After completing the task, some of the things you could try are (optional):\n",
    "* Implement different cost functions (binary cross-entropy)\n",
    "* Implement different activation functions (tanh, ReLU, softmax)\n",
    "* Incorporate these changes in the neural netwok code so that you can select the loss / activation function\n",
    "* Play with the hyper-paramters!\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "TASK 2\n",
    "INSTRUCTIONS:\n",
    "* Go through the documentaation of scikit from:\n",
    "  https://scikit-image.org/docs/stable/\n",
    "  focus more on the neural network modules\n",
    "  https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html (Neural Network Classifier)\n",
    "  https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html  (Neural Network Regressor)\n",
    "* Go through the MNIST dataset given here:\n",
    "  http://yann.lecun.com/exdb/mnist/\n",
    "  It can also be downloaded directly using scikit:\n",
    "  https://scikit-learn.org/0.19/datasets/mldata.html\n",
    "  But this seems to be deprecated, you could use a workaround given here:\n",
    "  https://stackoverflow.com/questions/47324921/cant-load-mnist-original-dataset-using-sklearn\n",
    "* Build a simple neural network (using scikit) and train it to recognize handwritten digits using the MNIST datasetself.\n",
    "  Make sure that you are able to vsualize the different aspects of the network, play around with the hyper-parameters and\n",
    "  try to get the best possible accuracy and report your accuracy on the ML-SIG group / channel\n",
    "  Remember to test different hyper-parameters on the validation set and to report the accuracy from the test set\n",
    "  https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7\n",
    "\"\"\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import mnist\n",
    "\n",
    "\n",
    "\n",
    "train_images = mnist.train_images()\n",
    "train_labels = mnist.train_labels()\n",
    "test_images = mnist.test_images()\n",
    "test_labels = mnist.test_labels()\n",
    "\n",
    "train_images = (train_images.reshape((-1,784)))/255\n",
    "test_images = (test_images.reshape((-1,784)))/255\n",
    "\n",
    "def log(x):\n",
    "    return 1 / ( 1+ np.exp(-1*x))\n",
    "def d_log(x):\n",
    "    return log(x) * (1 - log(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "def d_tanh(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "def ReLu(x):\n",
    "    mask = (x>0) * 1.0\n",
    "    return mask *x\n",
    "def d_ReLu(x):\n",
    "    mask = (x>0) * 1.0\n",
    "    return mask \n",
    "\n",
    "def arctan(x):\n",
    "    return np.arctan(x)\n",
    "def d_arctan(x):\n",
    "    return 1 / (1 + x ** 2)\n",
    "\n",
    "\"\"\"\n",
    "Other Common activation functions are:\n",
    "* tanh\n",
    "* ReLU\n",
    "* Softmax\n",
    "Read more about these at:\n",
    "https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n",
    "\"\"\"\n",
    "\n",
    "def activation(z, derivative=False):\n",
    "    \"\"\"\n",
    "    Sigmoid activation function:\n",
    "    It handles two modes: normal and derivative mode.\n",
    "    Applies a pointwise operation on vectors\n",
    "    Parameters:\n",
    "    ---\n",
    "    z: pre-activation vector at layer l\n",
    "        shape (n[l], batch_size)\n",
    "    Returns:\n",
    "    pontwize activation on each element of the input z\n",
    "    \"\"\"\n",
    "    sig = 1.0 / (1.0 + np.exp(-z))\n",
    "    if derivative:\n",
    "        return sig * (1 - sig)\n",
    "        # return the derivative of the sigmoid activation function\n",
    "    else:\n",
    "        return sig\n",
    "        # return the normal sigmoid activation function\n",
    "\n",
    "def cost_function_GAN(real,fake,is_discriminator=False):\n",
    "\n",
    "    if discrim:\n",
    "        #cost function for discriminator\n",
    "        cost = -np.mean(np.log(real) + np.log(1. -fake))\n",
    "        return cost\n",
    "    else:\n",
    "        #cost function for generator\n",
    "        cost = -np.mean(np.log(fake))\n",
    "        return cost\n",
    "    \n",
    "def cost_function(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Computes the Mean Square Error between a ground truth vector and a prediction vector\n",
    "    Parameters:\n",
    "    ---\n",
    "    y_true: ground-truth vector\n",
    "    y_pred: prediction vector\n",
    "    Returns:\n",
    "    ---\n",
    "    cost: a scalar value representing the loss\n",
    "    \"\"\"\n",
    "\n",
    "    if y_true.shape!=y_pred.shape:\n",
    "                y_pred=y_pred.T\n",
    "    n = y_pred.shape[0]\n",
    "    cost = (1./(2*n)) * np.sum((y_true - y_pred) ** 2)\n",
    "    return cost\n",
    "\n",
    "def cost_function_prime(real,fake,is_discriminator=False):\n",
    "    \"\"\"\n",
    "    Computes the derivative of the loss function w.r.t the activation of the output layer\n",
    "    Parameters:\n",
    "    ---\n",
    "    y_true: ground-truth vector\n",
    "    y_pred: prediction vector\n",
    "    Returns:\n",
    "    ---\n",
    "    cost_prime: derivative of the loss w.r.t. the activation of the output\n",
    "    shape: (n[L], batch_size)\n",
    "    \"\"\"\n",
    "    if discrim:\n",
    "        cost_prime = -np.mean((1/real)+(1/(1-fake)))\n",
    "        return cost_prime\n",
    "    else:\n",
    "        cost_prime = -np.mean((1/fake))\n",
    "        return cost_prime\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    '''\n",
    "    This is a custom neural netwok package built from scratch with numpy.\n",
    "    The Neural Network as well as its parameters and training method and procedure will\n",
    "    reside in this class.\n",
    "    Parameters\n",
    "    ---\n",
    "    size: list of number of neurons per layer\n",
    "    Examples\n",
    "    ---\n",
    "    >>> import NeuralNetwork\n",
    "    >>> nn = NeuralNetwork([2, 3, 4, 1])\n",
    "    This means :\n",
    "    1 input layer with 2 neurons\n",
    "    1 hidden layer with 3 neurons\n",
    "    1 hidden layer with 4 neurons\n",
    "    1 output layer with 1 neuron\n",
    "    '''\n",
    "\n",
    "    def __init__(self, size, seed=42):\n",
    "        '''\n",
    "        Instantiate the weights and biases of the network\n",
    "        weights and biases are attributes of the NeuralNetwork class\n",
    "        They are updated during the training\n",
    "        '''\n",
    "        self.seed = seed\n",
    "        np.random.seed(self.seed)\n",
    "        self.size = size\n",
    "        # biases are initialized randomly\n",
    "        self.biases = [np.random.rand(n, 1) for n in self.size[1:]]\n",
    "\n",
    "        self.weights = [np.random.rand(b, a) for a, b in zip(self.size[:-1], self.size[1:])]\n",
    "        \n",
    "\n",
    "        # initialize the weights randomly\n",
    "        \"\"\"\n",
    "        Be careful with the dimensions of the weights\n",
    "        The dimensions of the weight of any particular layer will depend on the\n",
    "        size of the current layer and the previous layer\n",
    "        Example: Size = [16,8,4,2]\n",
    "        The weight file will be a list with 3 matrices with shapes:\n",
    "        (8,16) for weights connecting layers 1 (16) and 2(8)\n",
    "        (4,8) for weights connecting layers 2 (8) and 4(4)\n",
    "        (2,4) for weights connecting layers 3 (4) and 4(2)\n",
    "        Each matrix will be initialized with random values\n",
    "        \"\"\"\n",
    "        # self.weights =\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "        Perform a feed forward computation\n",
    "        Parameters\n",
    "        ---\n",
    "        input: data to be fed to the network with\n",
    "        shape: (input_shape, batch_size)\n",
    "        Returns\n",
    "        ---\n",
    "        a: ouptut activation (output_shape, batch_size)\n",
    "        pre_activations: list of pre-activations per layer\n",
    "        each of shape (n[l], batch_size), where n[l] is the number\n",
    "        of neuron at layer l\n",
    "        activations: list of activations per layer\n",
    "        each of shape (n[l], batch_size), where n[l] is the number\n",
    "        of neuron at layer l\n",
    "        '''\n",
    "        a = input\n",
    "        pre_activations = []\n",
    "        activations = [a]\n",
    "        # TODO\n",
    "        # what does the zip function do?\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a) + b\n",
    "            a  = activation(z)\n",
    "            pre_activations.append(z)\n",
    "            activations.append(a)\n",
    "        return a, pre_activations, activations\n",
    "\n",
    "    \"\"\"\n",
    "    Resources:\n",
    "    https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/\n",
    "    https://hmkcode.github.io/ai/backpropagation-step-by-step/\n",
    "    \"\"\"\n",
    "    def compute_deltas(self, pre_activations, y_true, y_pred,is_discriminator=False):\n",
    "        \"\"\"\n",
    "        Computes a list containing the values of delta for each layer using\n",
    "        a recursion\n",
    "        Parameters:\n",
    "        ---\n",
    "        pre_activations: list of of pre-activations. each corresponding to a layer\n",
    "        y_true: ground truth values of the labels\n",
    "        y_pred: prediction values of the labels\n",
    "        Returns:\n",
    "        ---\n",
    "        deltas: a list of deltas per layer\n",
    "        \"\"\"\n",
    "\n",
    "        # initialize array to store the derivatives\n",
    "        delta = [0] * (len(self.size) - 1)\n",
    "\n",
    "        #TODO\n",
    "        # Calculate the delta for each layer\n",
    "        # This is the first step in calculating the derivative\n",
    "        #The last layer is calculated as derivative of cost function *  derivative of sigmoid ( pre-activations of last layer )\n",
    "        delta[-1] = cost_function_prime(y_true, y_pred,is_discriminator=is_discriminator) * activation(pre_activations[-1], derivative=True)\n",
    "\n",
    "        #TODO\n",
    "        # Recursively calculate delta for each layer from the previous layer\n",
    "        for l in range(len(deltas) - 2, -1, -1):\n",
    "\n",
    "            # deltas of layer l depend on the weights of layer l and l+1 and on the sigmoid derivative of the pre-activations of layer l\n",
    "            # Note that we use a dot product when multipying the weights and the deltas\n",
    "            # Check their shapes to ensure that their shapes conform to the requiremnts (You may need to transpose some of the matrices)\n",
    "            # The final shape of deltas of layer l must be the same as that of the activations of layer l\n",
    "            # Check if this is true\n",
    "            delta[l] = np.dot(delta[l+1], self.weights[l]) * activation(pre_activations[l], derivative=True)\n",
    "            #I HAVE COMMENTED THIS PART OUT AS ACTIVATIONS WASN'T PASSED\n",
    "            #if np.transpose(delta[l]) != np.transpose(activations[l]):\n",
    "             #   print(\"error in computing deltas!!!!!!!!\")\n",
    "        return deltas\n",
    "\n",
    "    def backpropagate(self, deltas, pre_activations, activations):\n",
    "        \"\"\"\n",
    "        Applies back-propagation and computes the gradient of the loss\n",
    "        w.r.t the weights and biases of the network\n",
    "        Parameters:\n",
    "        ---\n",
    "        deltas: list of deltas computed by compute_deltas\n",
    "        pre_activations: a list of pre-activations per layer\n",
    "        activations: a list of activations per layer\n",
    "        Returns:\n",
    "        ---\n",
    "        dW: list of gradients w.r.t. the weight matrices of the network\n",
    "        db: list of gradients w.r.t. the biases (vectors) of the network\n",
    "        \"\"\"\n",
    "        dW = []\n",
    "        db = []\n",
    "        deltas = [0] + deltas\n",
    "        for l in range(1, len(self.size)):\n",
    "            # TODO\n",
    "            # Compute the derivatives of the weights and the biases from the delta values calculated earlier\n",
    "            # dW_temp depends on the activations of layer l-1 and the deltas of layer l\n",
    "            # dB_temp depends only on the deltas of layer l\n",
    "            # Again be careful of the dimensions and ensure that the dW matrix has the same shape as W\n",
    "            dW_temp = delta[l] * activation(pre_activations[l], derivative=True) * activations[l-1]\n",
    "            db_temp = delta[l] * activation(pre_activations[l], derivative=True)\n",
    "            dW.append(dW_temp)\n",
    "            db.append(np.expand_dims(db_temp.mean(axis=1), 1))\n",
    "        return dW, db\n",
    "\n",
    "    def plot_loss(self,epochs,train,test):\n",
    "        \"\"\"\n",
    "        Plots the loss function of the train test data measured every epoch\n",
    "        Parameters:\n",
    "        ---\n",
    "        epochs: number of epochs for training\n",
    "        train: list of losses on the train set measured every epoch\n",
    "        test: list of losses on the test set measured every epoch\n",
    "        \"\"\"\n",
    "\n",
    "        plt.subplot(211)\n",
    "        plt.title('Training Cost (loss)')\n",
    "        plt.plot(range(epochs),train)\n",
    "\n",
    "        plt.subplot(212)\n",
    "        plt.title('Test Cost (loss)')\n",
    "        plt.plot(range(epochs),test)\n",
    "\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "        plt.show()\n",
    "\n",
    "    def train(self, X, y, batch_size, epochs, learning_rate, validation_split=0.2, print_every=10,is_discriminator=False):\n",
    "        \"\"\"\n",
    "        Trains the network using the gradients computed by back-propagation\n",
    "        Splits the data in train and validation splits\n",
    "        Processes the training data by batches and trains the network using batch gradient descent\n",
    "        Parameters:\n",
    "        ---\n",
    "        X: input data\n",
    "        y: input labels\n",
    "        batch_size: number of data points to process in each batch\n",
    "        epochs: number of epochs for the training\n",
    "        learning_rate: value of the learning rate\n",
    "        validation_split: percentage of the data for validation\n",
    "        print_every: the number of epochs by which the network logs the loss and accuracy metrics for train and validations splits\n",
    "        plot_every: the number of epochs by which the network plots the decision boundary\n",
    "        Returns:\n",
    "        ---\n",
    "        history: dictionary of train and validation metrics per epoch\n",
    "            train_acc: train accuracy\n",
    "            test_acc: validation accuracy\n",
    "            train_loss: train loss\n",
    "            test_loss: validation loss\n",
    "        This history is used to plot the performance of the model\n",
    "        \"\"\"\n",
    "        history_train_losses = []\n",
    "        history_train_accuracies = []\n",
    "        history_test_losses = []\n",
    "        history_test_accuracies = []\n",
    "        \n",
    "        \n",
    "        beta_1,beta_2,eps = 0.9,0.999,0.00000001\n",
    "        # TODO\n",
    "        # Read about the train_test_split function\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X.T, y.T, test_size=validation_split, )\n",
    "        x_train, x_test, y_train, y_test = x_train.T, x_test.T, y_train.T, y_test.T\n",
    "\n",
    "        epoch_iterator = range(epochs)\n",
    "        #FOR GAN, WE DEFINE WEIGHTS AND BIASES HERE\n",
    "        Generator_input = 1000\n",
    "        \n",
    "        hidden_input= 128\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        D_W1 = np.random.normal(size=(784,hidden_input),scale=(1. / np.sqrt(784 / 2.)))   *0.002\n",
    "        \n",
    "        D_b1 = np.zeros(hidden_input)\n",
    "\n",
    "        D_W2 = np.random.normal(size=(hidden_input,1),scale=(1. / np.sqrt(hidden_input / 2.)))     *0.002\n",
    "        \n",
    "        D_b2 = np.zeros(1)\n",
    "\n",
    "\n",
    "        G_W1 = np.random.normal(size=(G_input,hidden_input),scale=(1. / np.sqrt(G_input / 2.)))   *0.002\n",
    "\n",
    "        G_b1 = np.zeros(hidden_input)\n",
    "\n",
    "        \n",
    "        G_W2 = np.random.normal(size=(hidden_input,784),scale=(1. / np.sqrt(hidden_input / 2.)))  *0.002\n",
    "\n",
    "        G_b2 = np.zeros(784)\n",
    "        for iter in epoch_iterator:\n",
    "\n",
    "            random_int = np.random.randint(len(images) - 5)\n",
    "            current_image = np.expand_dims(images[random_int],axis=0)\n",
    "\n",
    "            # Func: Generate The first Fake Data\n",
    "                Z = np.random.uniform(-1., 1., size=[1, G_input])\n",
    "                Gl1 = Z.dot(G_W1) + G_b1\n",
    "                Gl1A = arctan(Gl1)\n",
    "                Gl2 = Gl1A.dot(G_W2) + G_b2\n",
    "                Gl2A = ReLu(Gl2)\n",
    "\n",
    "                current_fake_data = log(Gl2)\n",
    "                \n",
    "                \n",
    "                #Forward Feed for Real data\n",
    "                \n",
    "                Dl1_r = current_image.dot(D_W1) + D_b1\n",
    "                Dl1_rA = ReLu(Dl1_r)\n",
    "                Dl2_r = Dl1_rA.dot(D_W2) + D_b2\n",
    "                Dl2_rA = log(Dl2_r)\n",
    "                # Func: Forward Feed for Fake Data\n",
    "                Dl1_f = current_fake_data.dot(D_W1) + D_b1\n",
    "                Dl1_fA = ReLu(Dl1_f)\n",
    "                Dl2_f = Dl1_fA.dot(D_W2) + D_b2\n",
    "                Dl2_fA = log(Dl2_f)\n",
    "                \n",
    "                D_cost = cost_function_GAN(Dl2_rA,Dl2_fA,is_discriminator=True)\n",
    "                \n",
    "                # Func: Gradient\n",
    "                grad_fake_w2a =  1/(1.0- Dl2_fA)\n",
    "                grad_f_w2_partb =  d_log(Dl2_f)\n",
    "                grad_f_w2_partc =   Dl1_fA\n",
    "                grad_f_w2 =       grad_f_w2_part_3.T.dot(grad_f_w2_part_1 * grad_f_w2_part_2) \n",
    "                grad_f_b2 = grad_f_w2_part_1 * grad_f_w2_part_2\n",
    "\n",
    "                grad_f_w1_parta =  (grad_f_w2_part_1 * grad_f_w2_part_2).dot(D_W2.T)\n",
    "                grad_f_w1_partb =  d_ReLu(Dl1_f)\n",
    "                grad_f_w1_partc =   current_fake_data\n",
    "                grad_f_w1 =       grad_f_w1_part_3.T.dot(grad_f_w1_part_1 * grad_f_w1_part_2) \n",
    "                grad_f_b1 =      grad_f_w1_part_1 * grad_f_w1_part_2\n",
    "\n",
    "                grad_r_w2_part_1 =  - 1/Dl2_rA\n",
    "                grad_r_w2_part_2 =  d_log(Dl2_r)\n",
    "                grad_r_w2_part_3 =   Dl1_rA\n",
    "                grad_r_w2 =       grad_r_w2_part_3.T.dot(grad_r_w2_part_1 * grad_r_w2_part_2) \n",
    "                grad_r_b2 =       grad_r_w2_part_1 * grad_r_w2_part_2\n",
    "\n",
    "                grad_r_w1_part_1 = (grad_r_w2_part_1 * grad_r_w2_part_2).dot(D_W2.T)\n",
    "                grad_r_w1_part_2 =  d_ReLu(Dl1_r)\n",
    "                grad_r_w1_part_3 =  current_image\n",
    "                grad_r_w1 = grad_r_w1_part_3.T.dot(grad_r_w1_part_1 * grad_r_w1_part_2) \n",
    "                grad_r_b1 = grad_r_w1_part_1 * grad_r_w1_part_2\n",
    "\n",
    "                grad_w1 =grad_f_w1 + grad_r_w1\n",
    "                grad_b1 =grad_f_b1 + grad_r_b1\n",
    "\n",
    "                grad_w2 =grad_f_w2 + grad_r_w2\n",
    "                grad_b2 =grad_f_b2 + grad_r_b2\n",
    "                \n",
    "                b1 = beta_1 * b1 + (1 - beta_1) * grad_weight1\n",
    "                a1 = beta_2 * a1 + (1 - beta_2) * grad_weight1 ** 2\n",
    "\n",
    "                a2 = beta_1 * b2 + (1 - beta_1) * grad_bias1\n",
    "                a2 = beta_2 * a2 + (1 - beta_2) * grad_bias1 ** 2\n",
    "                \n",
    "                D_W1 = D_W1 - (learning_rate / (np.sqrt(a1 /(1-beta_2) ) + eps)) * (b1/(1-beta_1))\n",
    "                D_B1 = D_b1 - (learning_rate / (np.sqrt(a2 /(1-beta_2) ) + eps)) * (b2/(1-beta_1))\n",
    "\n",
    "                #forward-generator\n",
    "                Z = np.random.uniform(-1., 1., size=[1, Generator_input])\n",
    "                Gl1 = Z.dot(G_W1) + G_B1\n",
    "                Gl1A =activation(Gl1)\n",
    "                Gl2 = Z.dot(G_W2) + G_B2\n",
    "                Gl2A =activation(Gl2)\n",
    "\n",
    "                current_fake_data = log(Gl2A)\n",
    "\n",
    "                Discriminator_layer_1 = current_fake_data.dot(D_W1) + D_B1\n",
    "                Discriminator_layer_1_A =activation(Discriminator_layer_1)\n",
    "                Discriminator_layer_2 = current_fake_data.dot(D_W2) + D_B2\n",
    "                Discriminator_layer_2_A =activation(Discriminator_layer_2)\n",
    "                G_cost = cost_function_GAN(Discriminator_layer_2_A)\n",
    "                \n",
    "                if  d_log(Discriminator_layer_1).shape[1]!=D_W1.T.shape[0]:\n",
    "                D_W1.T=D_W1\n",
    "                \n",
    "                grad_G_weight1_a = ((-1/Discriminator_layer_1_A) * d_log(Discriminator_layer_1).dot(Discriminator_Weight_1.T) * (activation(Discriminator_layer_1))).dot(Discriminator_Weight_1)\n",
    "                grad_G_weight1_b = d_log(Generator_layer_1)\n",
    "                grad_G_weight1_c = Z\n",
    "                grad_G_weight1 = grad_G_weight1_c.T.dot(grad_G_weight1_a * grad_G_weight1_a)\n",
    "                grad_G_bias1 = grad_G_weight1_a * grad_G_weight1_b\n",
    "                \n",
    "                grad_G_w2_part_1 = (grad_G_w3_part_1 * grad_G_w3_part_2).dot(G_W3.T)\n",
    "                grad_G_w2_part_2 = d_ReLu(Gl2)\n",
    "                grad_G_w2_part_3 = Gl1A\n",
    "                grad_G_w2 = grad_G_w2_part_3.T.dot(grad_G_w2_part_1 * grad_G_w2_part_2)\n",
    "                grad_G_b2 = (grad_G_w2_part_1 * grad_G_w2_part_2)\n",
    "\n",
    "                #gradient\n",
    "                b3 = beta_1 * b3 + (1 - beta_1) * grad_G_weight1\n",
    "                a3 = beta_2 * a3 + (1 - beta_2) * grad_G_weight1 ** 2\n",
    "\n",
    "                b4 = beta_1 * b4 + (1 - beta_1) * grad_G_bias1\n",
    "                a4 = beta_2 * a4 + (1 - beta_2) * grad_G_bias1 ** 2\n",
    "                a5 = beta_1 * a5 + (1 - beta_1) * grad_G_w2\n",
    "                b5 = beta_2 * b5 + (1 - beta_2) * grad_G_w2 ** 2\n",
    "\n",
    "                a6 = beta_1 * a6 + (1 - beta_1) * grad_G_b2\n",
    "                b6 = beta_2 * b6 + (1 - beta_2) * grad_G_b2 ** 2\n",
    "\n",
    "                G_W1 = G_W1 - (learning_rate / (np.sqrt(a3 /(1-beta_2) ) + eps)) * (b3/(1-beta_1))\n",
    "                G_b1 = G_b1 - (learning_rate / (np.sqrt(a4 /(1-beta_2) ) + eps)) * (b4/(1-beta_1))\n",
    "                \n",
    "                G_W2 = G_W2 - (learing_rate / (np.sqrt(v7 /(1-beta_2) ) + eps)) * (m7/(1-beta_1))\n",
    "                G_b2 = G_b2 - (learing_rate / (np.sqrt(v8 /(1-beta_2) ) + eps)) * (m8/(1-beta_1))\n",
    "                \n",
    "                if h%10 == 0:\n",
    "\n",
    "                Z = np.random.uniform(-1., 1., size=[16, Generator_input]) \n",
    "\n",
    "                Generator_layer_1 = Z.dot(G_W1) + G_b1\n",
    "                Generator_layer_1_A =activation(Generator_layer_1)\n",
    "                present_fake_data = log(Generator_layer_1)\n",
    "\n",
    "                fig = plot(present_fake_data)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for e in epoch_iterator:\n",
    "            if x_train.shape[1] % batch_size == 0:\n",
    "                n_batches = int(x_train.shape[1] / batch_size)\n",
    "            else:\n",
    "                n_batches = int(x_train.shape[1] / batch_size ) - 1\n",
    "\n",
    "            x_train, y_train = shuffle(x_train.T, y_train.T)\n",
    "            x_train, y_train = x_train.T, y_train.T\n",
    "\n",
    "            batches_x = [x_train[:, batch_size*i:batch_size*(i+1)] for i in range(0, n_batches)]\n",
    "            batches_y = [y_train[:, batch_size*i:batch_size*(i+1)] for i in range(0, n_batches)]\n",
    "\n",
    "            train_losses = []\n",
    "            train_accuracies = []\n",
    "\n",
    "            test_losses = []\n",
    "            test_accuracies = []\n",
    "\n",
    "            dw_per_epoch = [np.zeros(w.shape) for w in self.weights]\n",
    "            db_per_epoch = [np.zeros(b.shape) for b in self.biases]\n",
    "\n",
    "            for batch_x, batch_y in zip(batches_x, batches_y):\n",
    "                batch_y_pred, pre_activations, activations = self.forward(batch_x)\n",
    "                deltas = self.compute_deltas(pre_activations, batch_y, batch_y_pred)\n",
    "                dW, db = self.backpropagate(deltas, pre_activations, activations)\n",
    "                for i, (dw_i, db_i) in enumerate(zip(dW, db)):\n",
    "                    dw_per_epoch[i] += dw_i / batch_size\n",
    "                    db_per_epoch[i] += db_i / batch_size\n",
    "\n",
    "                batch_y_train_pred = self.predict(batch_x)\n",
    "\n",
    "                train_loss = cost_function(batch_y, batch_y_train_pred)\n",
    "                train_losses.append(train_loss)\n",
    "                train_accuracy = accuracy_score(batch_y.T, batch_y_train_pred.T)\n",
    "                train_accuracies.append(train_accuracy)\n",
    "\n",
    "                batch_y_test_pred = self.predict(x_test)\n",
    "\n",
    "                test_loss = cost_function(y_test, batch_y_test_pred)\n",
    "                test_losses.append(test_loss)\n",
    "                test_accuracy = accuracy_score(y_test.T, batch_y_test_pred.T)\n",
    "                test_accuracies.append(test_accuracy)\n",
    "\n",
    "\n",
    "            # weight update\n",
    "\n",
    "            # TODO\n",
    "            # What does the enumerate function do?\n",
    "            for i, (dw_epoch, db_epoch) in enumerate(zip(dw_per_epoch, db_per_epoch)):\n",
    "                # TODO\n",
    "                # Update the weights using the backpropagation algorithm implemented earlier\n",
    "                # W = W - learning_rate * derivatives (dW)\n",
    "                # b = b - learning_rate * derivatives (db)\n",
    "                self.weights[i] = self.weights[i] - learning_rate * dw_epoch\n",
    "                self.biases[i] = self.biases[i] - learning_rate * db_epoch\n",
    "\n",
    "            history_train_losses.append(np.mean(train_losses))\n",
    "            history_train_accuracies.append(np.mean(train_accuracies))\n",
    "\n",
    "            history_test_losses.append(np.mean(test_losses))\n",
    "            history_test_accuracies.append(np.mean(test_accuracies))\n",
    "\n",
    "\n",
    "            if e % print_every == 0:\n",
    "                print('Epoch {} / {} | train loss: {} | train accuracy: {} | val loss : {} | val accuracy : {} '.format(\n",
    "                    e, epochs, np.round(np.mean(train_losses), 3), np.round(np.mean(train_accuracies), 3),\n",
    "                    np.round(np.mean(test_losses), 3),  np.round(np.mean(test_accuracies), 3)))\n",
    "\n",
    "        self.plot_loss(epochs,train_loss,test_loss)\n",
    "\n",
    "        history = {'epochs': epochs,\n",
    "                   'train_loss': history_train_losses,\n",
    "                   'train_acc': history_train_accuracies,\n",
    "                   'test_loss': history_test_losses,\n",
    "                   'test_acc': history_test_accuracies\n",
    "                   }\n",
    "        return history\n",
    "\n",
    "    def predict(self, a):\n",
    "        '''\n",
    "        Use the current state of the network to make predictions\n",
    "        Parameters:\n",
    "        ---\n",
    "        a: input data, shape: (input_shape, batch_size)\n",
    "        Returns:\n",
    "        ---\n",
    "        predictions: vector of output predictions\n",
    "        '''\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            z = np.dot(w, a) + b\n",
    "            a = activation(z)\n",
    "        predictions = (a > 0.5).astype(int)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7292e06ddcc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#input for generator is random noise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mg_input\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_images' is not defined"
     ]
    }
   ],
   "source": [
    "d_input=train_images\n",
    "d_label=[1 for j in range(train_images.shape[0])\n",
    "#input for generator is random noise\n",
    "\n",
    "g_input= [np.random.random() for i in range(784)]\n",
    "#initially loss is high as generator outputs garbage \n",
    "\n",
    "g_output= g_input\n",
    "\n",
    "#loss functions for discriminator and generator\n",
    "\n",
    "d_loss=cost_function(pred_real,pred_fake,is_discriminator=True)\n",
    "g_loss=cost_function(pred_real,pred_fake)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
